---
title: "Regression Project"
author: "Yong Pui Tung"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
subtitle: Predictive Modeling of Sports Car Prices - Exploring Factors and Model Performance
---
# Table of Content

1. Introduction
2. Dataset & Source
3. Data Description
4. Methodology
5. Data Preparation
6. Descriptive Statistics
7. Full Model Fitting
8. Linear Regression Analyses
9. Conclusion
10. Appendix

# 1. Introduction

Sports car are commonly seen as powerful, fast, luxurious. It is usually considered as high-end vehicles. According to an article written by Mckinsey & Company in 2022, the sales of luxury vehicle have surpassed the mass market. In addition to traditional comfort and safety features, customers are increasingly seeking sports cars with enhanced functionality, such as improved engines, acceleration time, and horsepower, among other features.

In this project, the focus is on analyzing the Sports Car Prices dataset using the statistical software RStudio. **The aim is to conduct in-depth linear regression analyses to identify the most suitable model for predicting the price of sports cars based on their functionality and technical specifications.**

By developing an accurate regression model, car manufacturers, dealerships, and buyers can benefit from estimating the value of sports cars based on their specific features and characteristics. Furthermore, this study provides valuable insights into the factors that contribute to the pricing of sports cars, shedding light on how these factors impact the overall market value of these vehicles.

# 2. Dataset & Source

The Sports Car Prices Dataset used in this study was obtained from the Kaggle dataset repository. The dataset provides comprehensive information on various attributes of sports cars, including their prices. The dataset used in this analysis is publicly accessible and can be found at the following URL: https://www.kaggle.com/datasets/rkiattisak/sports-car-prices-dataset. It offers a valuable resource for investigating the factors influencing sports car prices and conducting regression analysis to gain insights into the relationships between these factors and the pricing of sports cars.

# 3. Data Description

The data set we analyzes in this report contains information on sports cars and their prices. The data is collected in 2023 and has 1,007 observations. Each observation contains information about the prices and 7 features of various sports cars. The response variable in this dataset is the price of the car in USD, while the regressor variables include year of production, engine size in liters, horsepower, torque in pound-feet and the time in second it takes for the sports car to accelerate from 0 to 60 miles per hour. Significance level is set to 0.05 (5%).

There is 1 target variable and 7 regressor variables. For the 7 regressor variables, there are 4 numerical and 3 categorical variables.

**Target feature:**

1.	Price (in USD): The price of the sports car in US dollars, which represents the cost of purchasing the car. Prices in this dataset range from  $25,000 to $5,200,000.

**Descriptive feature:**

**Numerical**

2.	Engine Size (L): The size of the sports car's engine in liters, which represents the volume of the engine's cylinders. It ranges from 1.5L to 8.4L.

3.	Horsepower: The horsepower of the sports car, which represents the power output of the car's engine. It ranges from 181 to 1,600. Some cars use electric engines.

4.	Torque (lb-ft): The torque of the sports car in pound-feet, which represents the rotational force generated by the engine. It ranges from 151 to 1,300.

5.	0-60 MPH Time (seconds): The time it takes for the sports car to accelerate from 0 to 60 miles per hour, which is a common measure of acceleration and performance. It ranges from 2.1 to 6.5 seconds.

**Categorical**

6.	Car Make: The manufacturer of the sports car, which represents the brand or company that produced the car.

7.	Car Model: The model of the sports car, which represents the specific version or variant of the car produced by the manufacturer.

8.	Year: The year of production of the sports car, which indicates the model year when the car was first introduced or made available for purchase.

# 4. Methodology

For this project, we will split our data set to training and test data sets with a ratio of 7:3, meaning that 70% of our observations is the training set and 30% is the testing set. We will apply a few feature selection methods in order to find the best features for prediction. Then, we will create a regression model using training set and evaluate its performance using the test set.

The following assumptions will be tested and verified:

1. The price of the sports car is inversely proportional to engine size.
2. The price of the sports car is directly proportional to horsepower.
3. The price of the sports car is directly proportional to torque.
4. The price of the sports car is inversely proportional to 0-60 MPH Time.

In addition, we will use the selected regression model to make predictions of sports car prices and assess the model's performance.

# 5. Data Preparation

We will firstly clean and preprocess the data before analyzing.

## 5.1 Load Packages

```{r message=FALSE, warning=FALSE}
library(leaps)
library(ggplot2)
library(GGally)
library(olsrr)
library(car)
library(stats)
library(tidyverse)
library(TSA)
library(QuantPsyc)
library(lmtest)
library(forecast)
```

The customized function 'MASE' calculates the Mean absolute scaled error, a measure of the accuracy of the prediction.
  
```{r Functions}

# Mean absolute scaled error
MASE = function(observed , fitted ){
  # observed: Observed series on the forecast period
  # fitted: Forecast values by your model
  Y.t = observed
  n = length(fitted)
  e.t = Y.t - fitted
  sum = 0 
  for (i in 2:n){
    sum = sum + abs(Y.t[i] - Y.t[i-1] )
  }
  q.t = e.t / (sum/(n-1))
  MASE = data.frame( MASE = mean(abs(q.t)))
  return(list(MASE = MASE))
}
```

## 5.2 Data Cleaning

We observed that there are missing values called "N/A" and "-" in the dataset, so we specify the character strings to be treated as missing values.

```{r message=FALSE, warning=FALSE}
# Import data set using read.csv
sportscar <- read.csv("Sportcarprice.csv", na.strings = c("N/A", "-"))

# Check the first 6 rows of the data
head(sportscar)
```

In this project, the focus of the analysis is on understanding the relationship between the functionality and technical specifications of sports cars and their prices. By excluding the car brand and model as a predictor, the analysis aims to explore the influence of specific features and characteristics on pricing, regardless of the brand. 

This approach allows for a more direct examination of how features like engine size, acceleration time, and horsepower etc impact the pricing of sports cars, providing actionable insights for manufacturers, dealerships, and buyers. Therefore, we will drop the Car.make and Car.model columns.

```{r}
sportscar_2 <- sportscar[, !(names(sportscar) %in% c("Car.Make", "Car.Model"))]
head(sportscar_2)
```

### Handling Missing Values

Data set is imported. Now we will check if there is any missing value using is.na().

```{r}
# Check for missing values in all columns
sapply(sportscar_2, function(x) sum(is.na(x)))
```

It is found that there are 11 missing values in Engine Size and 4 missing value in Torque column. We will omit this value since we are in situations where the missing data (which is related to price) is unlikely to be imputed accurately.

```{r}
# Omit missing values
data_clean <- na.omit(sportscar_2)

# Check if missing values are omitted
sapply(data_clean, function(x) sum(is.na(x)))
```

### Check the Data Types

str() function is used to check the data type of all columns in a dataset. It will print a summary of the structure of the Sports Car dataset, including the data type of each column.

```{r}
str(data_clean)
```

After removing all missing values, we have 994 observations. The data types are all correct except "Engine Size". It should be numeric but it is identified by R as "character". Therefore, we check the unique values in Engine Size column to see if there is any character value.

```{r}
# Print unique values in the "Engine Size" column of the data frame
unique(data_clean$Engine.Size..L.)
```

It is found that there are other types of character values such as "Electric" and "Hybrid" in "Engine Size". Since it is not accurate to impute electric or hybrid car engine having a value of 0 or other values to engine size, we will remove these rows with "Electric" and "Hybrid" in this report. Our report is studying only conventional sports car except electric and hybrid cars. If we can find accurate data about engine size of electric and hybrid cars, we can do further studies in the future.

Also, an engine size of "0" is likely not a valid or normal value, as it does not make practical sense for a physical engine to have zero size. It could be an error or missing data.

```{r}
# Convert from character to numeric
data_clean$Engine.Size..L. <- as.numeric(data_clean$Engine.Size..L.)

# Remove all NA values
data_clean2 <- data_clean[!is.na(data_clean$Engine.Size..L.), ]

# Mark engine size equals to 0 as NA
data_clean2[data_clean2$Engine.Size..L.==0,] <- NA

# Remove all NA values
data_clean2 <- data_clean2[!is.na(data_clean2$Engine.Size..L.), ]
```


```{r}
data_clean2$X0.60.MPH.Time..seconds. <- as.numeric(data_clean2$X0.60.MPH.Time..seconds.)
data_clean2$Horsepower <- as.integer(data_clean2$Horsepower)
data_clean2$Torque..lb.ft. <- as.integer(data_clean2$Torque..lb.ft.)
data_clean2$Price..in.USD. <- as.integer(gsub(",", "", data_clean2$Price..in.USD.))

# Check the updated data set
str(data_clean2)
```

After removing these rows, there are 947 observations.

### Data Conversion

We convert the categorical "Year" variable to numeric "YearsSinceProduce". It can provide a more meaningful representation of the data and help capture the age or depreciation of the car. By subtracting the "Year" from the current year, we can calculate the number of years since the car was manufactured. 

Transforming the"Year" variable to "YearsSinceProduce" provides a variable that represents the age of the car model, independent of the specific year. This can facilitate the interpretation of regression coefficients by quantifying the number of years elapsed since the car was produced or released. It also ensures that the model remains applicable over time, as the "YearsSinceProduce" variable will consistently represent the number of years elapsed since the car was produced or released regardless of when the analysis is conducted.

```{r}
# Configure current year
current_year <- as.integer(format(Sys.Date(), "%Y"))

# Calculate car age and add a column
data_clean2$YearsSinceProduce <- current_year - data_clean2$Year

# Remove column "Year"
data_clean2 <- subset(data_clean2, select = -Year)
head(data_clean2)
```

# 6. Descriptive Statistics

## 6.1 Summary Statistics

In this section, we will apply these descriptive statistical techniques to our dataset, focusing on **summary statistics, histograms, and correlation matrices**. By examining these measures and visualizations, we can uncover valuable insights and patterns, which will enhance our understanding of the data and lay the groundwork for further analysis and modeling.

```{r}
# Print the summary statistics using the summary function
summary(data_clean2)
```

For engine size, it ranges from 1.5L to 8.4L, with a median of 4.0L.

For horsepower, it ranges from 181 to 1600, with a median of 580.

For torque, it ranges from 151 to 1300 lb-ft, with a median of 505.

For 0-60 MPH Time (seconds), it is the time it takes for the sports car to accelerate from 0 to 60 mph ranges from 2.1 to 6.5 seconds, with a median value of 3.5 seconds.

For YearsSinceProduce, this variable represents the number of years since the sports car was produced. The minimum value is 1, indicating  last year, and the maximum value is 59, with a median of 3.

For car price, the prices of the sports cars range from 25,000 to 5,200,000, with a median value of 120,000. The mean is higher than the median, it generally indicates that the distribution is positively skewed. We can visualize it using histogram.

## 6.2 Histogram

In this section, we will explore histograms of the numeric variables in our dataset. By examining the shape, spread, and peaks of the histograms, we can gain a deeper understanding of the underlying distribution of each variable. This information can help us uncover important characteristics such as symmetry, skewness, or the presence of outliers. By visualizing the data in this way, we can better comprehend its distributional properties and make informed decisions about further analysis and modeling.

```{r}
# Plot histogram of price
ggplot(data_clean2, aes(x = Price..in.USD.)) +
  geom_histogram(binwidth = 100000, color = "black", fill = "purple") +
  labs(title = "Histogram of Sports Car Price", x = "Sports Car Price", y = "Frequency")
```

From the histogram of the sports car price, we observe a **highly skewed distribution towards the right**. This indicates that the majority of sports cars in our dataset have relatively lower prices, while there are a few outliers with exceptionally high prices. The presence of these outliers suggests the existence of luxury or high-performance sports cars in our dataset, which significantly influence the overall distribution of prices. By examining the histogram, we can visually identify the extent of the skewness and the concentration of prices within different price ranges. This information will be valuable for understanding the price distribution and identifying any potential patterns or trends that may exist within our data.

Then, we check the histogram for the regressors variables.

```{r}
# Plot histogram of engine size
ggplot(data_clean2, aes(x = Engine.Size..L.)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "skyblue") +
  labs(title = "Histogram of Engine Size", x = "Engine Size", y = "Frequency")
```

From the histogram of the engine size variable, we can observe a **slightly skewed distribution towards the right**. This indicates that the majority of sports cars in our dataset have engine sizes that are relatively small, while there are fewer cars with larger engine sizes. The skewness towards the right suggests that there may be a few sports cars with exceptionally large engine sizes, which contribute to the overall shape of the distribution.

```{r}
# Plot histogram of horsepower
ggplot(data_clean2, aes(x = Horsepower)) +
  geom_histogram(binwidth = 30, color = "black", fill = "red") +
  labs(title = "Histogram of Horsepower", x = "Horsepower", y = "Frequency")
```

The histogram of the horsepower variable reveals a **right-skewed distribution**. This indicates that the majority of sports cars in our dataset have relatively lower horsepower values, while a few cars exhibit exceptionally high horsepower. The presence of outliers with large horsepower values contributes to the elongated tail on the right side of the histogram.

```{r}
# Plot histogram of torque
ggplot(data_clean2, aes(x = Torque..lb.ft.)) +
  geom_histogram(binwidth = 30, color = "black", fill = "lightgreen") +
  labs(title = "Histogram of Torque", x = "Torque", y = "Frequency")
```

Similarly, the histogram of the torque variable exhibits a similar pattern to that of horsepower. It is **slightly skewed to the right**, indicating that most sports cars in our dataset have relatively lower torque values, while a few cars possess significantly higher torque. The presence of outliers with large torque values contributes to the elongated tail on the right side of the histogram. B

```{r}
# Plot histogram of MPH time
ggplot(data_clean2, aes(x = X0.60.MPH.Time..seconds.)) +
  geom_histogram(binwidth = 0.3, color = "black", fill = "darkred") +
  labs(title = "Histogram of MPH Time", x = "MPH Time", y = "Frequency")
```

The histogram of the 0-60 MPH Time variable reveals a **slightly right-skewed distribution**. This indicates that the majority of sports cars in our dataset have relatively faster acceleration times, with shorter durations to reach 60 miles per hour. 

```{r}
# Plot histogram of years since car production
ggplot(data_clean2, aes(x = YearsSinceProduce)) +
  geom_histogram(binwidth = 1, color = "black", fill = "orange") +
  labs(title = "Histogram of Years Since Car Production", x = "Years Since Car Production", y = "Frequency")
```

The histogram of the Years Since Car Production variable displays a **highly right-skewed distribution**. This suggests that the majority of sports cars in our dataset were produced relatively new, within the last three years. The concentration of cars in the more recent production years is evident from the higher frequency of observations in the corresponding bins on the right side of the histogram. A clear outlier is present, featuring a notably extended duration of 59 years since the car's production.

## 6.3 Correlation Matrix

Multicollinearity refers to the situation where two or more predictors in a regression model are highly correlated with each other. In other words, it is the presence of a linear relationship between two or more predictors. We will use **correlation matrix** of the predictor variables using ggpairs() function. Then we examine the correlation matrix to identify any strongly correlated pairs of predictor variables.

```{r, warning=FALSE, message=FALSE}
create_ggpairs <- function(data) {
  # Select the numeric variables for analysis
  numeric_vars <- c("Engine.Size..L.", "Horsepower", "Torque..lb.ft.", "X0.60.MPH.Time..seconds.", "YearsSinceProduce", "Price..in.USD.")
  numeric_data <- data[, numeric_vars]
  
  # Create the scatter plot matrix using ggpairs
  ggpairs(numeric_data)
}

create_ggpairs(data_clean2)
```

From the table, the numeric variables are all skewed to the right. **Horsepower and Toque are strongly related** with a correlation coefficient of $0.920$. It is a strong indication of multicollinearity. We will confirm multicollinearity by the Variance Inflation Factor ($VIF$) later.

# 7. Full Model Fitting

As part of our model building strategies, we will firstly fit the full model, then perform residual analysis to see if any data transformation is needed. To conduct a linear regression analysis we have the following assumptions:

- Linearity: The relationship between response variable and regressor variables is approximately linear
- Homoscedasticity: Errors are of constant variance
- Autocorrelation: Errors are uncorrelated
- Normality: Errors are normally distributed
- Residuals have zero mean

We perform full model fitting using linear regression. The objective is to predict the "Price..in.USD." of sports cars based on various predictor variables. The lm() function is used to fit the model, and the summary() function is applied to obtain a summary of the model, including coefficient estimates and evaluation metrics.

```{r}
# Full Model Fitting
model_full <- lm(Price..in.USD.~., data=data_clean2)
summary(model_full)
```
The fitted multiple linear regression model is $\hat Y= -2230065 - 78287 \hspace{0.1 cm}  EngineSize +2871 \hspace{0.1 cm} Horsepower +  492 \hspace{0.1 cm} Torque +  247903 \hspace{0.1 cm} MPH time + 25107 \hspace{0.1 cm} YearsSinceProduce + \varepsilon$ where the independent error terms $\varepsilon$ is assumed to follow a normal distribution with mean 0 and equal variance. Y is dependent or response variable. $EngineSize$, $Horsepower$, $Torque$, $MPH time$ and $YearsSinceProduce$ are independent variables.

Null Hypothesis ($H_0$): There is no significant relationship between the predictors and the response variable.

Alternative Hypothesis ($H_A$): There is a significant relationship between the predictors and the response variable.

Based on the obtained p-value < 2.2e-16 , which is less than the significance level $0.05$, we have sufficient evidence to reject the null hypothesis. **This indicates that there is a statistically significant relationship between the predictors (independent variables) included in the regression model and the response variable (dependent variable). We can conclude that the overall model is significant.**

The adjusted R-squared value is $0.6625$, meaning that approximately 66.25% of the variability in the response variable is accounted for by the predictor variables in the regression model.

## Multicollinearity

Multicollinearity is a problem that may impact the estimates of the individual regression coefficients. To test multicollinearity, we should also use the Variance Inflation Factor ($VIF$). The Variance Inflation Factor (VIF) is a measure of the amount of multicollinearity present in a set of predictor variables. A high VIF value indicates a high correlation between a predictor variable and the other variables in the model.

If $VIF$ value is greater than 5, it indicates a significant and strong correlation between the predictor variable and the other variables in the model. The regressors that have high VIFs have poorly estimated regression coefficients

```{r message=FALSE, warning=FALSE}
vif(lm(Price..in.USD.~., data=data_clean2))
```

As we can see, **the VIF of Horsepower (8.036152) and Torque (6.816369) are considered significant in multicollinearity.** Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, making it difficult to determine the unique contribution of each variable to the response variable. We will address it during linear regression analysis stage.

## Residual Analysis

Residuals analysis is important for regression models because it helps to check the assumptions of the model and the quality of the fit.

```{r}
par(mfrow=c(2,2))
plot(model_full)
```

### Influential observation

The **Residuals vs Leverage** plot helps us find influential cases using Cook's distance. There are no outlying values at the upper right corner or lower right corner. The spots at the upper right corner or lower right corner are places where cases can be influential against a regression line.

### Linearity

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most residuals are more or less spreading around the horizontal line. It can be an indication that we **do not have non-linear relationships**, but we will need to check the partial residual plots.

One key assumption of multiple linear regression is that there exists a linear relationship between each predictor variable and the response variable. To check this assumption, we create a partial residual plot, which displays the residuals of one predictor variable against the response variable. We can use crPlots() function to create partial residual plots.

```{r message=FALSE, warning=FALSE}
crPlots(model_full)
```

The dotted blue line shows the expected residuals if the relationship between the predictor and response variable is linear. The pink line shows the actual residuals. If the two lines differ, it indicates evidence of non-linear relationship. We observe that the two lines are following each other closely. It is likely that there are no violations of the linearity assumption in the full model.

### Autocorrelation

We check if the residuals have any autocorrelation by ACF and PACF plots.

```{r}
par(mfrow=c(1,2))
acf(model_full$residuals, main = "The ACF of the residuals")
pacf(model_full$residuals, main = "The PACF of the residuals")
```

The presence of 1-2 significant lags in the ACF and PACF plots of residuals implies that there might be some remaining temporal patterns or dependencies in the model residuals that are not adequately captured by the current model. 

To check auto-correlation statistically, we use Durbin-Watson test. The Durbin-Watson test is a statistical test used to check for the presence of autocorrelation in the residuals of a regression model. It examines whether there is a systematic pattern of correlation between consecutive residuals.

Null Hypothesis ($H_0$): There is no autocorrelation present in the residuals.

Alternative Hypothesis ($H_A$): There is autocorrelation present in the residuals.

```{r}
durbinWatsonTest(model_full)
```

Based on the Durbin-Watson test result, the p-value is 0.002. Since the p-value is less than $0.05$, we have sufficient evidence to reject the null hypothesis. This suggests that **there is significant autocorrelation present in the residuals** of the model.

### Normality

The QQ plot is a valuable tool for assessing the normality of a dataset. It compares the observed quantiles of the data to the expected quantiles of a normal distribution. If the points on the plot closely follow a straight line, it suggests that the data follows a normal distribution. Deviations from the straight line indicate departures from normality, such as skewness or heavy tails. 

```{r}
qqnorm(model_full$residuals, xlab="Normal Scores")
qqline(model_full$residuals)
```

We observe deviated right tails in the QQ plot. To test normality statistically, we can use the Shapiro-Wilk test.

Null Hypothesis ($H_0$): The residuals of the full model are normally distributed.

Alternative Hypothesis ($H_A$): The residuals of the full model are not normally distributed.

```{r}
shapiro.test(model_full$residuals) 
```

Based on the Shapiro-Wilk test result, where the p-value is less than $0.05$, we reject the null hypothesis of normality for the residuals of the full model. Since the p-value is less than $0.05$, we have sufficient evidence to reject the null hypothesis. This suggests that **the residuals of the full model do not follow a normal distribution.**

### Heteroscedasticity

The ncvTest, or non-constant variance test, is used to assess whether there is evidence of heteroscedasticity in a regression model. It examines the relationship between the residuals and the fitted values of the model. A significant result from the ncvTest indicates the presence of heteroscedasticity, suggesting that the variance of the errors is not constant across different levels of the predictors.

Null Hypothesis ($H_0$): The variance of the residuals is constant (homoscedasticity).

Alternative Hypothesis ($H_A$): The variance of the residuals is not constant (heteroscedasticity).

```{r}
ncvTest(model_full)
```

Since the p-value is less than $0.05$, we have strong evidence to reject the null hypothesis. This suggests that there is **significant evidence of heteroscedasticity in the residuals of the model.** To deal with heteroscedasticity, we can apply data transformation.

## Logarithmic Transformation

As we find violations of normality and heteroscedasticity, we consider data transformations (e.g. logarithmic and Box-Cox transformation) to address non-normality. As the data is highly right skewed observed from Histogram of Sports Car Price, we will firstly consider logarithmic transformation.

```{r}
# Take log of Price
lm.log <- lm(log(Price..in.USD.) ~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds., data = data_clean2)
summary(lm.log)
```
The fitted multiple linear regression model is $\hat Y = 11.2521751 - 0.1217822 \hspace{0.1 cm}  EngineSize +0.0035026 \hspace{0.1 cm} Horsepower -0.2826933 \hspace{0.1 cm} MPH time + 0.0648458 \hspace{0.1 cm} YearsSinceProduce + \varepsilon$ where the independent error terms $\varepsilon$ is assumed to follow a normal distribution with mean 0 and equal variance. Y is dependent or response variable. $EngineSize$, $Horsepower$, $MPH time$ and $YearsSinceProduce$ are independent variables. All variables are significant.

```{r}
# Plot histogram of log Price
ggplot(data_clean2, aes(x = log(Price..in.USD.) )) +
  geom_histogram(binwidth = 0.1, color = "purple", fill = "purple") +
  labs(title = "Histogram of Log-Transformed Sports Car Price", x = "Log-Transformed Sports Car Price", y = "Frequency")
```

After log transformation, the distribution is obviously less right skewed when compared to the original histogram of Sports Car Price, but it is still positively skewed.

```{r}
# Residual Plots
par(mfrow=c(2,2))
plot(lm.log)
```

### Influential observation

The **Residuals vs Leverage** plot helps us find influential cases using Cook's distance. A point in the right bottom corner of the Residuals vs Leverage plot suggests that the corresponding observation has a notable impact on the model, potentially influencing the regression coefficients. 

### Linearity

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most residuals are more or less spreading around the horizontal line. It can be an indication that we **do not have non-linear relationships**, but we will need to check the partial residual plots.

One key assumption of multiple linear regression is that there exists a linear relationship between each predictor variable and the response variable. To check this assumption, we create a partial residual plot, which displays the residuals of one predictor variable against the response variable. We can use crPlots() function to create partial residual plots.

```{r}
crPlots(lm.log)
```

The dotted blue line shows the expected residuals if the relationship between the predictor and response variable is linear. The pink line shows the actual residuals. If the two lines differ, it indicates evidence of non-linear relationship. We observe that the two lines are pretty close except YearsSinceProduce.

### Autocorrelation

We check if the residuals have any autocorrelation by ACF and PACF plots.

```{r}
par(mfrow=c(1,2))
acf(lm.log$residuals, main = "The ACF of the residuals")
pacf(lm.log$residuals, main = "The PACF of the residuals")
```

From ACF and PACF plot, there are still 2-3 significant lags. Auto-correlation exists. To check auto-correlation statistically, we use Durbin-Watson test. It examines whether there is a systematic pattern of correlation between consecutive residuals.

Null Hypothesis ($H_0$): There is no autocorrelation present in the residuals.

Alternative Hypothesis ($H_A$): There is autocorrelation present in the residuals.

```{r}
durbinWatsonTest(lm.log)
```

Based on the Durbin-Watson test result, the p-value is $0.07$. Since the p-value is larger than $0.05$, we do not have sufficient evidence to reject the null hypothesis. This suggests that **there is no autocorrelation present in the residuals** of the model.

### Normality

The QQ plot is a valuable tool for assessing the normality of a dataset. It compares the observed quantiles of the data to the expected quantiles of a normal distribution. If the points on the plot closely follow a straight line, it suggests that the data follows a normal distribution.

```{r}
qqnorm(lm.log$residuals, xlab="Normal Scores")
qqline(lm.log$residuals)
```

We still observe many points deviate from the straight line in the QQ plot. It suggests that the data does not follow a normal distribution. To test normality statistically, we can use the Shapiro-Wilk test.

Null Hypothesis ($H_0$): Errors are normally distributed.

Alternative Hypothesis ($H_A$): Errors are not normally distributed.

```{r}
shapiro.test(lm.log$residuals)
```

Unfortunately, the p-value is still very low. Normality has no improvement. We reject the null hypothesis. **There is strong evidence that errors are not normally distributed.**

### Heteroscedasticity

The ncvTest, or non-constant variance test, is used to assess whether there is evidence of heteroscedasticity in a regression model. A significant result from the ncvTest indicates the presence of heteroscedasticity, suggesting that the variance of the errors is not constant across different levels of the predictors.

Null Hypothesis ($H_0$): The variance of the residuals is constant (homoscedasticity).

Alternative Hypothesis ($H_A$): The variance of the residuals is not constant (heteroscedasticity).

```{r}
ncvTest(lm.log)
```

Since the p-value is less than $0.05$, we have strong evidence to reject the null hypothesis. This suggests that there is **significant evidence of heteroscedasticity in the residuals of the model.** To deal with heteroscedasticity, we can apply data transformation or weighted least squares.

Logarithmic transformation does not work effectively to correct normality and heteroscedasticity. Thus, we will try another approach to fix normality.

## Box-Cox Transformation

To address the issues identified in the residual analysis, we can try using the box-cox function from the MASS package to perform the Box-Cox transformation. The Box-Cox transformation helps identify the optimal power transformation that maximizes the likelihood of the data being normally distributed. 

```{r}
# Perform the Box-Cox transformation
transformed_data <- boxcox(Price..in.USD. ~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds., data = data_clean2)

# Extract the optimal lambda value
lambda <- transformed_data$x[which.max(transformed_data$y)]

# Apply the Box-Cox transform to the response variable
data_clean2$Price_transformed <- ((data_clean2$Price..in.USD.^lambda) - 1) / lambda
```
```{r}
lambda
```

Lamba is found to be -0.1818182. 

```{r}
# Plot histogram of BC transformed car price
ggplot(data_clean2, aes(x = Price_transformed)) +
  geom_histogram(binwidth = 0.01, color = "blue", fill = "blue") +
  labs(title = "Histogram of BC Transformed Sports Car Price", x = "BC Transformed Sports Car Price", y = "Frequency")
```

The histogram of Sports Car Price has greatly improved from the original histogram after Box-Cox transformation. It looks more normally distributed than before.

Next we fit the model with the transformed response variable.

```{r}
model_full_transformed <- lm(Price_transformed ~ YearsSinceProduce + Horsepower + Torque..lb.ft. + Engine.Size..L. + X0.60.MPH.Time..seconds., data=data_clean2)
summary(model_full_transformed)
```
All predictor variables are significant with a p-value lower than 0.05. The overall model is significant.

We will perform residual analysis again with the transformed data.

```{r}
# Residual Plots
par(mfrow=c(2,2))
plot(model_full_transformed)
```

### Influential observation

The **Residuals vs Leverage** plot helps us find influential cases using Cook's distance. There is still one outlying value at the bottom right corner.

### Linearity

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most residuals are more or less spreading around the horizontal line. It can be an indication that we **do not have non-linear relationships**, but we will need to check the partial residual plots.

```{r}
crPlots(model_full_transformed)
```

The pink line isfollowing the blue line, which indicates that it is likely that there are no violations of the linearity assumption in this model.

### Autocorrelation

We check if the residuals have any autocorrelation by ACF and PACF plots.

```{r}
par(mfrow=c(1,2))
acf(model_full_transformed$residuals, main = "The ACF of the residuals")
pacf(model_full_transformed$residuals, main = "The PACF of the residuals")
```

There are still 1-3 significant lags left in ACF and PACF plots of residuals. Autocorrelation in the residuals might indicate that our model is missing relevant predictor variables or that there are unaccounted systematic patterns in the data. To check statistically, we will use Durbin-Watson test.

Null Hypothesis ($H_0$): There is no autocorrelation present in the residuals.

Alternative Hypothesis ($H_A$): There is autocorrelation present in the residuals.

```{r}
durbinWatsonTest(model_full_transformed)
```

Based on the Durbin-Watson test result, the p-value is $0.888$. Since the p-value is larger than $0.05$, we do not have sufficient evidence to reject the null hypothesis. **This suggests that there is no autocorrelation present in the residuals of the model.**

### Normality

The QQ plot is a valuable tool for assessing the normality of a dataset. It compares the observed quantiles of the data to the expected quantiles of a normal distribution. If the points on the plot closely follow a straight line, it suggests that the data follows a normal distribution.

```{r}
qqnorm(model_full_transformed$residuals, xlab="Normal Scores")
qqline(model_full_transformed$residuals)
```

There are still positive tails deviated from the line. The Box-Cox transformed data is still skewed to the right.

Null Hypothesis ($H_0$): Errors are normally distributed.

Alternative Hypothesis ($H_A$): Errors are not normally distributed.

```{r}
shapiro.test(model_full_transformed$residuals)
```

Based on the Shapiro-Wilk test result, although there is improvement in the normality in terms of p-value, the p-value is still way smaller than $0.05$, we have sufficient evidence to reject the null hypothesis. **This suggests that errors are not normally distributed.**

### Heteroscedasticity

The ncvTest, or non-constant variance test, is used to assess whether there is evidence of heteroscedasticity in a regression model. A significant result from the ncvTest indicates the presence of heteroscedasticity, suggesting that the variance of the errors is not constant across different levels of the predictors.

Null Hypothesis ($H_0$): Errors have a constant variance.

Alternative Hypothesis ($H_A$): Errors have a non-constant variance.

```{r}
ncvTest(model_full_transformed)
```

From ncvTest, the p-value is greater than $0.05$, we do not have evidence to reject the null hypothesis. This suggests that errors have a non-constant variance. **This implies that constant error variance assumption is not violated.**

We will continue our analyses using the Box-Cox transformed data because error variance assumption becomes not violated. However, we should be aware that residuals are not normal.

# 8. Linear Regression Analyses

## 8.1 Test Train Split

In this section, a test-train split is performed to partition the dataset into training and testing subsets. The aim is to use the training subset to build a predictive model and evaluate its performance on the independent testing subset. The split is done using a 70-30 ratio, where 70% of the data is allocated to the training set and the remaining 30% is assigned to the test set. To ensure reproducibility, a random seed of 999 is set before sampling the data. The resulting training set (train) will be used for model development, while the test set (test) will be used for evaluating the model's performance.

```{r}
# 70% as training
sample_size = floor(0.7 * nrow(data_clean2))

# Set the seed to make partition reproducible
set.seed(999)
train_set <- sample(seq_len(nrow(data_clean2)), size = sample_size)

train <- data_clean2[train_set,]
test <- data_clean2[-train_set,]
```

## 8.2 Modelling the Data

In this section, we will perform all possible regressions and then select the best model from the candidates models.

Firstly, we fit a full multiple linear regression model using the lm() function using Car Age, Horsepower, Torque, Engine Size, MPH time as predictors in the dataset. Box-Cox transformed price is the response variable. Then we will make use of features selection methods to select features that best explains the variation in sport car prices. Three candidate models are considered and the performance and validity of results of each model is evaluated using significance tests and model adequacy tests.

### Model 1 - Full Model

Our first model is built using the full set of predictor variables in the data.

```{r}
# Model 1 - Full Model using train data
model1 <- lm(Price_transformed ~ YearsSinceProduce + Horsepower + Torque..lb.ft. + Engine.Size..L. + X0.60.MPH.Time..seconds., data=train)
summary(model1)
```
The fitted multiple linear regression model is $\hat Y= 4.875e+00 - 1.264e-02 \hspace{0.1 cm}  EngineSize + 4.128e-04  \hspace{0.1 cm} Horsepower -1.361e-04 \hspace{0.1 cm} Torque -4.433e-02 \hspace{0.1 cm} MPH time + 9.820e-03 \hspace{0.1 cm} YearsSinceProduce + \varepsilon$ where the independent error terms $\varepsilon$ is assumed to follow a normal distribution with mean 0 and equal variance. Y is dependent or response variable. $EngineSize$, $Horsepower$, $Torque$, $MPH time$ and $YearsSinceProduce$ are independent variables.

All predictor variables are significant at 5% level of significance. The overall model has a p-value less than $0.05$. This suggests strong evidence that all predictors in the model are significantly associated with the response variable. This model has a adjusted R-square of $0.6977$, meaning that 69.77% of the variability in the response variable (sports car price) can be explained by the predictor variables (car age, engine size, torque, horsepower, and MPH time). The adjusted R-squared takes into account the number of predictors in the model and adjusts the R-squared value accordingly.

#### ANOVA

We could use ANOVA (Analysis of Variance) to test the significance of model 1.

$H_0$: The fit of intercept only model and the current model is the same, meaning that additional variables do not provide values taken together.

$H_A$ The fit of intercept only model is significantly less when compared to our current model.

```{r}
summary_1 <- ols_regress(Price_transformed ~ YearsSinceProduce + Horsepower + Torque..lb.ft.+ Engine.Size..L. + X0.60.MPH.Time..seconds., data = train)
summary_1
```

The ANOVA table gives us a F-statistics of $305.606$ and p-value 0.0000, suggesting that **the regression is significant** at 5% level of significance.

#### Model Adequacy

We will check assumption of multiple linear regression for model 1 to confirm validity of the results. Residuals Analysis is important for regression models because it helps to check the assumptions of the model and the quality of the fit.We check the residual plot to assess the normality and constant variance assumptions.

**Linearity**

```{r}
plot(model1, which=1)
```

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most residuals are equally spread around the horizontal line. It is good indication that we **do not have non-linear relationships**.

One key assumption of multiple linear regression is that there exists a linear relationship between each predictor variable and the response variable. To check this assumption, we create a partial residual plot, which displays the residuals of one predictor variable against the response variable. We can use crPlots() function to create partial residual plots.

```{r message=FALSE, warning=FALSE}
crPlots(model1)
```

The dotted blue line shows the expected residuals if the relationship between the predictor and response variable is linear. The pink line shows the actual residuals. If the two lines differ, it indicates evidence of non-linear relationship. Overall speaking the pink line is following the dotted blue line well.

**Normality**

**QQ plot** shows if residuals are normally distributed. Most residuals follow the straight line, but some points deviate from the line at the both end of the data. The QQ-plot suggest that the residuals are violating normality at the tail end of the data.

```{r}
plot(model1, which=2)
```

We can also test the assumptions statistically. The Shapiro-Wilk test is a statistical test that checks whether a set of data follows a normal distribution. 

Null Hypothesis ($H_0$): Errors are normally distributed.

Alternative Hypothesis ($H_A$): Errors are not normally distributed.

```{r}
shapiro.test(model1$residuals) 
```

Based on the Shapiro-Wilk test result, the p-value is less than $0.05$, we reject the null hypothesis. We have sufficient evidence to reject the null hypothesis. This implies that  **normality error assumption is violated**.

**Autocorrelation**

We use ACF and PACF plots to check if there is autocorrelation left in the residuals.

```{r}
par(mfrow=c(1,2))
acf(model1$residuals, main = "The ACF of the residuals")
pacf(model1$residuals, main = "The PACF of the residuals")
```

There is still 1-2 significant lags left, but they are at late lag. Overall, there is not much auto-correlation left in the residuals. We will use Durbin-Watson test to test statistically. It is used to check for the presence of autocorrelation in the residuals of a regression model.The durbinWatsonTest() function takes the regression model and returns the lag one sample autocorrelation coefficient, $DW$ test statistics and p value.

Null Hypothesis ($H_0$): Errors are uncorrelated.

Alternative Hypothesis ($H_A$): Errors are correlated.

```{r}
durbinWatsonTest(model1)
```

Since the p-value is $>$ $0.05$, we do not have enough evidence to reject $H_0$ that there is no autocorrelation. This implies that **uncorrelated error assumption is not violated**.

**Error Variance**

```{r}
plot(model1, which=3)
```

**Scale-Location** plot shows if residuals are spread equally along the ranges of predictors. This is used for checking assumption of equal variance. The line is slightly horizontal with equally spread points. It may indicate homoscedasticity. To test error variance statistically, we can use ncvTest.

Null Hypothesis ($H_0$): Errors have a constant variance.

Alternative Hypothesis ($H_A$): Errors have a non-constant variance.

```{r}
ncvTest(model1)
```

Since the p-value is $>$ $0.05$, we cannot reject $H_0$. This implies that **constant error variance assumption is not violated**.

**Multicollinearity**

Variance inflation factor (VIF) is used to check multicollinearity of the model.

```{r}
vif(model1) 
```

As we can see, **the VIF of Horsepower (7.949121) and Torque (6.671375) are considered significant in multicollinearity.** Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, making it difficult to determine the unique contribution of each variable to the response variable. 

### Model 2 - Reduced Model

Edmondson (2011) stated that torque and horsepower are closely related and cannot be decoupled from each other. Horsepower is the product of torque and a couple of constants. With this strong evidence, we can safely respecify the model and remove one of these two variables from the model. We will remove the one with weaker evidence of association with the response variable.

```{r}
car::Anova(model1, type="III") # library(car) 
```

Among horsepower and torque variables, we will consider dropping the variable with the higher p-value, which in this case is "Torque..lb.ft." (p-value = 4.928e-05) based on ANOVA table. Here is our updated Model 2.

```{r}
# Remove Torque Variable
# Model 2
model2 <- lm(Price_transformed~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds., data=train)
summary(model2)
```
The fitted multiple linear regression model is $\hat Y= 4.875e+00 -1.492e-02 \hspace{0.1 cm}  EngineSize + 3.227e-04  \hspace{0.1 cm} Horsepower -4.427e-02 \hspace{0.1 cm} MPH time + 1.118e-02 \hspace{0.1 cm} YearsSinceProduce + \varepsilon$ where the independent error terms $\varepsilon$ is assumed to follow a normal distribution with mean 0 and equal variance. Y is dependent or response variable. $EngineSize$, $Horsepower$, $MPH time$ and $YearsSinceProduce$ are independent variables.

All predictor variables are significant at 5% level of significance. The overall model has a p-value less than $0.05$. This suggests strong evidence that all predictors in the model are significantly associated with the response variable. This model has a adjusted R-square of $0.6904$, meaning that 69.04% of the variability in the response variable (sports car price) can be explained by the predictor variables (car age, engine size, horsepower, and MPH time). The adjusted R-squared takes into account the number of predictors in the model and adjusts the R-squared value accordingly.

Now we will check the VIF to see if we have successfully handled multicollinearity.

```{r message=FALSE, warning=FALSE}
vif(lm(Price_transformed~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds., data=train))
```

After removing the torque variable, the VIF of all variables are considered insignificant in multicollinearity. We have addressed the multicollinearity problem.

#### ANOVA

We could use ANOVA (Analysis of Variance) to test the significance of model 2.

$H_0$: The fit of intercept only model and the current model is the same, meaning that additional variables do not provide values taken together.

$H_A$ The fit of intercept only model is significantly less when compared to our current model.

```{r}
summary_2 <- ols_regress(Price_transformed ~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds., data=train)
summary_2
```

The ANOVA table gives us a F-statistics of $369.003$ and p-value 0.0000, suggesting that the regression is significant at 5% level of significance.

#### Model Adequacy

We will check assumption of multiple linear regression for model 2 to confirm validity of the results. Residuals Analysis is important for regression models because it helps to check the assumptions of the model and the quality of the fit.

**Linearity**

```{r}
plot(model2, which=1)
```

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most residuals are equally spread around the horizontal line. It is positive indication that we **do not have non-linear relationships**.

One key assumption of multiple linear regression is that there exists a linear relationship between each predictor variable and the response variable. To check this **linearity** assumption, we create a partial residual plot, which displays the residuals of one predictor variable against the response variable. We can use crPlots() function to create partial residual plots.

```{r message=FALSE, warning=FALSE}
crPlots(model2)
```

The dotted blue line shows the expected residuals if the relationship between the predictor and response variable is linear. The pink line shows the actual residuals. If the two lines differ, it indicates evidence of non-linear relationship. We observe that the pink lines are following the dotted blue line well.

**Normality**

**QQ plot** shows if residuals are normally distributed. Most residuals follow the straight line, but some points deviate from the line at the both end of the data. The QQ-plot suggest that the residuals are violating normality at the tail end of the data.

```{r}
plot(model2, which=2)
```

We can also test the assumptions statistically. The Shapiro-Wilk test is a statistical test that checks whether a set of data follows a normal distribution. 

Null Hypothesis ($H_0$): Errors are normally distributed.

Alternative Hypothesis ($H_A$): Errors are not normally distributed.

```{r}
shapiro.test(model2$residuals) 
```

Based on the Shapiro-Wilk test result, the p-value is less than $0.05$, we reject the null hypothesis. We have sufficient evidence to reject the null hypothesis. This implies that  **normality error assumption is violated**.

**Autocorrelation**

ACF and PACF plots are used to check autocorrelation left in the residuals.

```{r}
par(mfrow=c(1,2))
acf(model2$residuals, main = "The ACF of the residuals")
pacf(model2$residuals, main = "The PACF of the residuals")
```

There is still 1 to 2 significant lags left in the residuals, but they are at late lag. Overall, there is not much auto-correlation left in the residuals. We will use Durbin-Watson test to test statistically. It is used to check for the presence of autocorrelation in the residuals of a regression model.The durbinWatsonTest() function takes the regression model and returns the lag one sample autocorrelation coefficient, $DW$ test statistics and p value.

Null Hypothesis ($H_0$): Errors are uncorrelated.

Alternative Hypothesis ($H_A$): Errors are correlated.

```{r}
durbinWatsonTest(model2)
```

Since the p-value is $>$ $0.05$, we do not have enough evidence to reject $H_0$ that there is no autocorrelation. This implies that **uncorrelated error assumption is not violated**.

**Error Variance**

```{r}
plot(model2, which=3)
```

**Scale-Location** plot shows if residuals are spread equally along the ranges of predictors. This is used for checking assumption of equal variance. The line is slightly horizontal with equally spread points. It may indicate homoscedasticity. To test error variance statistically, we can use ncvTest.

Null Hypothesis ($H_0$): Errors have a constant variance.

Alternative Hypothesis ($H_A$): Errors have a non-constant variance.

```{r}
ncvTest(model2)
```

Since the p-value is $>$ $0.05$, we cannot reject $H_0$. This implies that **constant error variance assumption is not violated**.

### Stepwise & All-possible Regression Approach

Stepwise elimination is used to select the variables that gives the best Akaike Information Criterion (AIC) value. Both ols_step_both_aic() and step() methods can be used to perform stepwise regression

ols_step_both_aic() performs a stepwise regression using both forward and backward selection based on the AIC criterion. It starts with an empty model and iteratively adds or removes variables until no further improvement in the AIC value is achieved. The final model selected is the one with the lowest AIC. Based on ols_step_both_aic() $AIC$-value approach, the variables that should be entered are all four predictor variables.

```{r}
ols_step_both_aic(model1)
```

It suggests that all five predictor variables should be included based on AIC approach.

Next we will try all-possible regression approach. We use $adjr2$ as our model selector criterion.

```{r}
all_possible_reg <-leaps::regsubsets(Price_transformed ~ YearsSinceProduce + Horsepower + Torque..lb.ft.+ Engine.Size..L. + X0.60.MPH.Time..seconds., data=train)
summary(all_possible_reg)
```

The resulting table provides information about the different models generated by the All Possible Subsets regression. It gives us the best models at each variable number. We can use graphical table to visualize the best subsets.

```{r}
plot(all_possible_reg, scale="adjr2") 
```

Based on the graph above, by adjusted \( R^2 \), the best model includes all predictor variables (variables that have black boxes at the highest adjusted \( R^2 \) value).

As the all-possible and stepwise regression approach consistently suggest that the full model (Model 1) is the best, it indicates that all the included variables are important for explaining the response variable. As a result, we do not have any new candidate model arise in this step.

### Model 3 - GLM

For model 3, we explore other distributions that are appropriate for our data. As our price data follows a right skewed distribution, we can try fitting a GLM regression model to the data using gamma distribution.

In the context of Generalized Linear Models (GLMs), the gamma distribution is a suitable choice for the response variable when certain characteristics are present. Specifically, it is commonly used when the dependent variable is continuous, positively skewed, and its mean is related to the predictor variables through a log link function. The gamma distribution accommodates situations where the relationship between the predictors and the response is expected to be multiplicative rather than additive. By modeling the data using the gamma distribution and a log link, the GLM can capture these characteristics and provide appropriate inference and interpretation of the results.

```{r}
# Use original Price data
glm.fit <- glm(Price..in.USD. ~ YearsSinceProduce + Horsepower + Engine.Size..L. + X0.60.MPH.Time..seconds.,
             family = Gamma(link="log"), data=train)
summary.glm(glm.fit)
```

Only horsepower and MPH time predictor variables are significant with p-value less than 0.05. We will check if the model deviance indicates that the GLM model is satisfactory.

$H_0$: The model fits the data well.
$H_A$: The model does not fit the data well.

```{r}
deviance(glm.fit)

# p value for the goodness of fit test
pchisq(glm.fit$deviance, df=glm.fit$df.residual, lower.tail=FALSE)
```

The deviance() function is used to calculate the deviance of a fitted generalized linear model (glm). The deviance is a measure of the lack of fit of the model to the observed data. A lower deviance value indicates a better fit of the model to the data, suggesting that the model explains a larger proportion of the variability in the response variable. 

The pchisq() function is then applied to obtain the p-value for the goodness of fit test using the deviance and the residual degrees of freedom. The chi-square test statistic of $226.43$ gives a p-value of $1$, indicating that the null hypothesis is plausible, and we can conclude that the GLM model provides an adequate fit to the data.

#### ANOVA

Next we perform a type 3 partial deviance analysis of the model parameters, to see if we should remove any regressors from the model.

```{r}
Anova(glm.fit,type = 3)
```

It indicates that Engine Size and YearsSinceProduce variables should be removed, as their p-values are greater than 0.05.

#### Model Adequacy

We will check assumption of multiple linear regression for GLM model to confirm validity of the results. Residuals Analysis is important for regression models because it helps to check the assumptions of the model and the quality of the fit.

```{r}
# residuals vs fitted
plot(glm.fit, which=1)
```

**Residuals vs Fitted** plot shows if residuals have non-linear patterns. Most of the residuals are equally spread around the horizontal line. It is good indication that we **do not have non-linear relationships**.

**Linearity**

To check linear relationship assumption, we create a partial residual plot, which displays the residuals of one predictor variable against the response variable. We can use crPlots() function to create partial residual plots.

```{r, warning=FALSE, message=FALSE}
crPlots(glm.fit)
```

The dotted blue line shows the expected residuals if the relationship between the predictor and response variable is linear. The pink line shows the actual residuals. If the two lines differ, it indicates evidence of non-linear relationship. The pink line is following the dotted blue line closely. **It is good indication that there is no non-linear relationship.**

**Normality**

**QQ plot** shows if residuals are normally distributed. Most residuals follow the straight line, but some points deviate from the line at the both end of the data. The QQ-plot clearly suggests that the residuals are violating normality at the tail end of the data.

```{r}
plot(glm.fit, which=2)
```

There are a lot of points deviated from the straight line. We can also test the assumptions statistically. The Shapiro-Wilk test is a statistical test that checks whether a set of data follows a normal distribution.

Null Hypothesis ($H_0$): Errors are normally distributed.

Alternative Hypothesis ($H_A$): Errors are not normally distributed.

```{r}
res_dev = residuals(glm.fit, type="deviance")
shapiro.test(res_dev) 
```

Based on the Shapiro-Wilk test result, the p-value is less than $0.05$, we reject the null hypothesis. We have sufficient evidence to reject the null hypothesis. This implies that  **normality error assumption is violated**.

**Autocorrelation**

```{r}
par(mfrow=c(1,2))
acf(res_dev, main = "The ACF of the residuals")
pacf(res_dev, main = "The PACF of the residuals")
```

Similar to model 1 and 2, there is 1-2 significant late lags. There is not much auto-correlation left in the residuals. We will use Durbin-Watson test to test statistically.

Null Hypothesis ($H_0$): Errors are uncorrelated.

Alternative Hypothesis ($H_A$): Errors are correlated.

```{r}
durbinWatsonTest(glm.fit)
```

The p-value is $<$ $0.05$, we have enough evidence to reject $H_0$ that there is no autocorrelation left in the residuals. This implies that **uncorrelated error assumption is violated**.

**Error Variance**

```{r}
plot(glm.fit, which=3)
```

**Scale-Location** plot shows if residuals are spread equally along the ranges of predictors. This is used for checking assumption of equal variance. The line is slightly horizontal with equally spread points. It may indicate homoscedasticity. 

**Multicollinearity**

Variance inflation factor (VIF) is used to check multicollinearity of the model.  

```{r}
vif(glm.fit) 
```

There is no multicollinearity in the model. They all have VIF equal or less than 3.268073.

## 8.3 Model Comparision  
  
Anova results show that all the models are significant at 5% level of significance. Although we have performed Box-Cox transformation, all three models fails to meet the normality assumption. 

Model 1 and Model 2 are chosen over GLM Model since **Model 1 and Model 2 show better residual characteristics. Uncorrelated error assumption is violated in GLM Model.** 

Also, only two variables are significant in GLM Model, but **all predictor variables are significant in the Model 1 and Model 2**. 

**Model 1 VS Model 2**

To compare two linear regression models and determine the better fit, we will assess their adjusted R-squared, F-statistic, RMSE, ANOVA, and press statistics. These metrics allow us to evaluate the models' explanatory power, overall significance, predictive accuracy, and ability to explain variation in the response variable. By considering these measures, we can make an informed comparison and select the model that best fits the data.

```{r}
# Create table to compare Model 1 and Model 2
Metrics <- c("Number of variables", "Multiple R-squared",	"Adjusted R-squared" ,
             "F-statistic","RMSE")
model_1_table <- c(round(summary_1$n-1,0), summary_1$rsq, summary_1$adjr, summary_1$f, sqrt(summary_1$mse))
model_2_table <- c(round(summary_2$n-1,0), summary_2$rsq, summary_2$adjr , summary_2$f , sqrt(summary_2$mse))
table <- data.frame(Metrics, model_1_table, model_2_table , stringsAsFactors=F)
colnames(table) <- c("Metrics", "Model 1", "Model 2")
table
```

Both Model 1 and Model 2 can explain approximately 69% of the variation in Sports Car Price, as shown from Adjusted R-squared value. Model 1 is able to explain 69.77% of variation in sports car price with 5 variables and Model 2 is explaining almost 69.04% of variation with 4 variables.

Model 1 has an F-statistic of 305, while Model 2 has a higher F-statistic of 369 The F-statistic measures the overall significance of the model. A higher F-statistic indicates a better overall fit of the model to the data.

Model 1 has an RMSE (Root Mean Squared Error) of 0.059, while Model 2 has a slightly higher RMSE of 0.060 The RMSE represents the average difference between the predicted values and the actual values, with lower values indicating better predictive performance.

**ANOVA**

The ANOVA test is used to compare the goodness-of-fit between two models. The resulting p-value helps determine whether there is sufficient evidence to reject the null hypothesis and conclude that there is a significant difference in the fit between the models.

Null Hypothesis($H_0$): There is no significant difference in the fit between Model 1 and Model 2.

Alternative Hypothesis($H_A$): There is a significant difference in the fit between Model 1 and Model 2.

```{r warning=FALSE, message=FALSE}
Comp_pvalue <- anova(model1, model2)
Comp_pvalue
```

The anova gives a p-value of $4.928e-05$ lower than 0.05, suggesting that the two models are significantly different. In the context of model comparison, a small p-value suggests that the additional predictors included in the model compared to the other significantly improve the fit of the model. It indicates that the variables included in the more complex model (Model 1) contributes significantly to explaining the variation in the response variable.

The model with the lowest values of PRESS indicates the best structure. The "Press" statistic is a measure of predictive accuracy for a regression model. It quantifies the sum of squared differences between the predicted values and the actual values for each observation, with one observation excluded at a time. 

```{r}
# Press statistics for Model 1
DAAG::press(model1) 

# Press statistics for Model 2
DAAG::press(model2)
```

Model 1 has a Press statistic of 2.319341, while Model 2 has a Press statistic of 2.368002. The lower the Press statistic, the better the predictive accuracy of the model. Therefore, Model 1 has a slightly better predictive accuracy than Model 2, as it has a lower Press statistic, which means that including the variable Torque to the model is significantly recommended. 

However. we should consider the fact that Model 1 has **Multicollinearity** issue supported by Variance inflation factor (VIF). Horsepower and Torque are highly correlated, making it difficult to determine the unique contribution of each variable to the response variable. After removal of Torque variable, Model 2 does not have **Multicollinearity** issue. Therefore, even though Model 1 has lower Press statistics and has significant difference in the fit, we consider using Model 2 in this report as a better fit for the sports car price data. Model 2 also has a higher F-statistic, indicating a better fit of the model.

Model 2 will be used for explaining the variations in sports car price and for predicting the sports car price in test data.

## Verifying The Assumptions And Making Predictions
  
The coefficients of the Model 2 are as follows.
  
```{r}
# coefficients of Model 2
model2$coefficients
```

It is evident from the coefficients of model 2 that:

1. The price of the sports car is inversely proportional to engine size.
2. The price of the sports car is directly proportional to horsepower.
3. The price of the sports car is inversely proportional to 0-60 MPH Time.

Since we have removed Torque variable, the relationship between price and torque is not testified. The rest of the assumptions regarding the model are verified and model 2 is then used to predict sports car price using test data.
  
```{r}
# Make prediction using model 2
pred <- as.data.frame(predict(model2, test[,c(1,2,4,6)],interval = "prediction"))

# Add the actual values from the test dataset to the 'pred' data frame
pred$actual_value <- test$Price_transformed

# Display the predicted and actual values
head(pred)
```
  
## Mean Absolute Scaled Error

Mean Absolute Scaled Error (MASE) is less sensitive to outliers when compared to other error metrics like Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE). It calculates the errors as a ratio, which reduces the influence of extreme values and makes it a robust measure of forecast accuracy.
 
```{r}
# Calculate MASE using customized function
MASE(pred$fit, pred$actual_value)
```

The MASE of the prediction using Model 2 is good with a value less than 1. Hence, the model can be regarded as sufficient in answering our research questions and predicting sports car prices.

## Root Mean Squared Error

```{r}
accuracy_data <- accuracy(pred$fit, pred$actual_value)
accuracy_data
```

The accuracy_data object contains several metrics that assess the accuracy of the predicted values compared to the actual values in the test set. For example, RMSE (Root Mean Squared Error) is the square root of the mean squared differences between the predicted and actual values, which measures the average magnitude of the prediction errors. MAE (Mean Absolute Error) is the mean absolute difference between the predicted and actual values, which measures the average absolute magnitude of the prediction errors.

In our case, the accuracy metrics indicate that the model's predictions have a small mean error (ME), low root mean squared error (RMSE), and mean absolute error (MAE). The mean percentage error (MPE) and mean absolute percentage error (MAPE) are also within acceptable ranges, indicating reasonable accuracy of the model's predictions on the test set.

# 9. Conclusion  
  
In this study, we aim to develop a regression model to **predict the price of conventional sports cars (excluding Electric or Hybrid sports car) based on their functionality and technical specifications.** Our analysis included numerical features such as engine size, horsepower, torque, 0-60 MPH time and years since production.

To address issues of non-normality, heteroscedasticity in the data, we performed log and Boxcox transformation. The Boxcox transformation yielded the best results, allowing us to proceed with model fitting. We split the dataset into training and testing sets to evaluate the performance of our models. We considered three regression models: **the full model (model 1), a model without the Torque variable to address multicollinearity (model 2), and a generalized linear model (GLM).**

Upon analyzing the models, we found that the GLM violated the assumption of uncorrelated errors, leading us to exclude it from further consideration. Between model 1 and model 2, although model 1 had a slightly higher adjusted R-squared, slightly lower RMSE, and lower Press statistic, **model 2 emerged as the preferred choice due to its ability to address multicollinearity issues and higher F-statistic.**

The coefficients of model 2 provided valuable insights into the relationship between the predictor variables and the price of sports cars. We observed that engine size and 0-60 MPH time were inversely proportional to the price, while horsepower had a direct relationship with the price. Using model 2, we made predictions of sports car prices on a test dataset. **The Mean Absolute Scaled Error (MASE) for the predictions was less than 1, indicating that the model performed well in estimating the prices of sports cars.**

Overall, our study demonstrates that considering the functionality and technical specifications of conventional sports cars can provide valuable insights into their pricing dynamics. However, it is important to acknowledge that the study focused solely on conventional sports cars and did not encompass electric or hybrid cars. Future research can explore the pricing dynamics of electric and hybrid sports cars separately to gain a comprehensive understanding of the entire sports car market.

# 10. Appendix

Edmondson, C. (2011). Torque or Horsepower? Finding the Shift Points. In Fast Car Physics. Johns Hopkins University Press.

McKinsey & Company. (2022). Five Trends Shaping Tomorrow's Luxury Car Market. Retrieved 1 Jun, 2023 from https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/five-trends-shaping-tomorrows-luxury-car-market

Rattanaporn, K. (2023). Sports Car Prices dataset (Kaggle). Retrieved May 15, 2023 from https://www.kaggle.com/datasets/rkiattisak/sports-car-prices-dataset